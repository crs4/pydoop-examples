#!/usr/bin/env bash

set -euo pipefail
[ -n "${DEBUG:-}" ] && set -x
this="${BASH_SOURCE-$0}"
this_dir=$(cd -P -- "$(dirname -- "${this}")" && pwd -P)
. "${this_dir}/../config.sh"

use_two_classes () {
    find "${1}" -mindepth 1 -maxdepth 1 -type d | tail -n +3 | xargs rm -rf
}

downsample () {
    local d=$1
    shopt -q -o xtrace && local reset_x=1
    set +x
    find "${d}" -mindepth 2 -maxdepth 2 -type f | while read f; \
	do [ $(( RANDOM % 20 )) -gt 1 ] && rm -f "${f}";
    done
    [ -n ${reset_x} ] && set -x
}

[ ${PY_VER} -le 2 ] && die "ERROR: Python 3 required"

NUM_MAPS="${NUM_MAPS:-4}"
OPTS=( "--num-maps" "${NUM_MAPS}" )
[ -n "${DEBUG:-}" ] && OPTS+=( "--log-level" "DEBUG" )

export LIBHDFS_OPTS="-Xmx512m"

pushd "${this_dir}"
img_dir="flower_photos"
bneck_dir="bottlenecks"
train_output="trained_models"
img_url="http://download.tensorflow.org/example_images/${img_dir}.tgz"
if [ ! -d "${img_dir}" ]; then
    curl "${img_url}" | tar xz
    if [ -n "${DEBUG:-}" ]; then
	use_two_classes "${img_dir}"
	downsample "${img_dir}"
    fi
fi
ensure_dfs_home
${HDFS} dfs -test -d "${img_dir}" || ${HADOOP} distcp -atomic "file://${PWD}/${img_dir}" "${img_dir}"
${HDFS} dfs -rm -r -f "${bneck_dir}" "${train_output}"
${PYTHON} genbnecks.py "${OPTS[@]}" "${img_dir}" "${bneck_dir}"
${PYTHON} retrain_subsets.py "${OPTS[@]}" --num-steps 400 "${bneck_dir}" "${train_output}"
popd
